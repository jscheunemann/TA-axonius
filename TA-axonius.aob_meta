{"basic_builder": {"appname": "TA-axonius", "friendly_name": "TA Axonius", "version": "1.0.41", "author": "Axonius", "description": "The official Axonius Splunk Technology Add-on", "theme": "#ffffff", "large_icon": "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", "small_icon": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAJKADAAQAAAABAAAAJAAAAAAqDuP8AAACJUlEQVRYCc1XPy8EQRTfI1EggoZEcToqovABJCokPoGaj+FTEJVOoSXRyhUqjY5GSDQ0GlFJ1vvt7bt7Nztv39yswUsm783783u/mZ3Zu82yrmyQysUo3cmV7LnF3RbJkAG2OZ5Kcx+p19BMOqS9mooJ4U7V9FUJgVwq+SBgufiePUKBl1Rda3AnlNhbqwxou8FxpT7aHdQPu7VL44EGb+F7dEu98FXgd8jG7RrV0/uRbTKZGOu9fjjY2vTgANsr1iMJ2lovctc5dD0eU4xojSRWSI7ML2yL0GGlornjqAnEOBVjpb5h4fpq4Ju1Cq24D3jGKqL4JA1fbUCpnXInwMfs9F4GcpkUXiemWLfMBMj32/eUtFQm3rZOntfNopqERoSIDFZfESIVjWvdskozduQH7Se2XU2xC9cXOo8mRCejrTbJsx01ZgTiCRnAseEmhHCYNTnVApY/+vAB+F8dahAqblMru4Hdlfy8yQ1jlBDNLzfojZCCMmeZNNd+DVFXm8qAUoeQWhFkZG1tMyuIH0IJJm2rVuZKe84qrIsfJyAETFWsW4aVaZKkNvY9ZJHBIkJyKoutI3TpZOOf3vyQjUBqgcY1DSlncqLZ+EPlflFeackN/HhvyUPeoXnvw5G3VTsrHG/Q31uq9sMj+/SW/I2z4CK3z7VT0XokYLcXz9XAdCo2hIszwwQGNJ8ROF3hmOv/qbmv58DXDDP9zTOFH1zuWzyRb7xN2fGjdyxsAAAAAElFTkSuQmCC", "visible": true, "tab_version": "4.0.0", "tab_build_no": "0", "build_no": 42}, "data_input_builder": {"datainputs": [{"index": "default", "sourcetype": "axonius_saved_query", "interval": "86400", "use_external_validation": true, "streaming_mode_xml": true, "name": "axonius_saved_query", "title": "Axonius Saved Query", "description": "", "type": "customized", "parameters": [{"name": "api_host", "label": "Axonius Host", "help_string": "The URL of the Axonius web host", "required": true, "format_type": "text", "default_value": "", "placeholder": "https://axonius.example.com", "type": "text", "value": "https://demo-latest.axonius.com"}, {"name": "api_key", "label": "API Key", "help_string": "The API Key from https://axonius.example.com/account -> API Key", "required": true, "format_type": "password", "default_value": "", "placeholder": "", "type": "password", "value": "71WkhjhZryS8qQrURAD6ilEI5cx2Ou_NhL41DP_i8Nw"}, {"name": "api_secret", "label": "API Secret", "help_string": "The API Secret from https://axonius.example.com/account -> API Key", "required": true, "format_type": "password", "default_value": "", "placeholder": "", "type": "password", "value": "VY7qvUkI47xZg9VmpRhFa6t6fm4L8qV-DK1gwII3krE"}, {"name": "entity_type", "label": "Entity Type", "help_string": "The entity type of the saved query, either devices or users", "required": true, "possible_values": [{"value": "devices", "label": "Devices"}, {"value": "users", "label": "Users"}], "format_type": "dropdownlist", "default_value": "devices", "placeholder": "", "type": "dropdownlist", "value": "devices"}, {"name": "saved_query", "label": "Saved Query", "help_string": "The name of the saved query", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "Ax - Get all devices"}, {"name": "page_size", "label": "Page Size", "help_string": "The number of asset entities to fetch during each API call, higher is quicker while lower takes less memory", "required": true, "format_type": "text", "default_value": "1000", "placeholder": "", "type": "text", "value": "1000"}, {"name": "standoff_ms", "label": "API Standoff (milliseconds)", "help_string": "The number of milliseconds to wait between successive API calls", "required": true, "format_type": "text", "default_value": "0", "placeholder": "", "type": "text", "value": "0"}, {"name": "dynamic_field_mapping", "label": "Dynamic Field Mapping", "help_string": "Rename fields using a JSON-formatted string, renaming occurs prior to data ingest", "required": false, "format_type": "text", "default_value": "", "placeholder": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}", "type": "text", "value": "{\"hostname\": \"host\", \"network_interfaces.manufacturer\": \"nic_manufacturer\"}"}, {"name": "shorten_field_names", "label": "Shorten Field Names", "help_string": "Truncate the field name prefix, if applicable (specific_data.data, adapters_data)", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}, {"name": "incremental_data_ingest", "label": "Incremental Data Ingest", "help_string": "Include only the entities that have a fetch timer newer than last collection", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": false}, {"name": "enforce_ssl_validation", "label": "Enforce SSL Validation", "help_string": "Enforce SSL certificate validation (the Splunk server's global certificate trust will be used if CA Bundle Path is left blank)", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}, {"name": "ssl_certificate_path", "label": "CA Bundle Path", "help_string": "The filesystem path to the CA bundle used for SSL certificate validation", "required": false, "format_type": "text", "default_value": "", "placeholder": "Path to CA bundle (Examples: C:/Certs/ca_bundle.pem or /home/splunk/ca_bundle.pem)", "type": "text", "value": ""}], "data_inputs_options": [{"type": "customized_var", "name": "api_host", "title": "Axonius Host", "description": "The URL of the Axonius web host", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": "https://axonius.example.com"}, {"type": "customized_var", "name": "api_key", "title": "API Key", "description": "The API Key from https://axonius.example.com/account -> API Key", "required_on_edit": false, "required_on_create": true, "format_type": "password", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_secret", "title": "API Secret", "description": "The API Secret from https://axonius.example.com/account -> API Key", "required_on_edit": false, "required_on_create": true, "format_type": "password", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "entity_type", "title": "Entity Type", "description": "The entity type of the saved query, either devices or users", "required_on_edit": false, "required_on_create": true, "possible_values": [{"value": "devices", "label": "Devices"}, {"value": "users", "label": "Users"}], "format_type": "dropdownlist", "default_value": "devices", "placeholder": ""}, {"type": "customized_var", "name": "saved_query", "title": "Saved Query", "description": "The name of the saved query", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "page_size", "title": "Page Size", "description": "The number of asset entities to fetch during each API call, higher is quicker while lower takes less memory", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "1000", "placeholder": ""}, {"type": "customized_var", "name": "standoff_ms", "title": "API Standoff (milliseconds)", "description": "The number of milliseconds to wait between successive API calls", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "0", "placeholder": ""}, {"type": "customized_var", "name": "dynamic_field_mapping", "title": "Dynamic Field Mapping", "description": "Rename fields using a JSON-formatted string, renaming occurs prior to data ingest", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}"}, {"type": "customized_var", "name": "shorten_field_names", "title": "Shorten Field Names", "description": "Truncate the field name prefix, if applicable (specific_data.data, adapters_data)", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "incremental_data_ingest", "title": "Incremental Data Ingest", "description": "Include only the entities that have a fetch timer newer than last collection", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "enforce_ssl_validation", "title": "Enforce SSL Validation", "description": "Enforce SSL certificate validation (the Splunk server's global certificate trust will be used if CA Bundle Path is left blank)", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "ssl_certificate_path", "title": "CA Bundle Path", "description": "The filesystem path to the CA bundle used for SSL certificate validation", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": "Path to CA bundle (Examples: C:/Certs/ca_bundle.pem or /home/splunk/ca_bundle.pem)"}], "code": "# encoding = utf-8\n\nimport datetime\nimport json\nimport os\nimport re\nimport requests\nimport sys\nimport time\n\nclass Config:\n    supported_minimum_version: str = \"4.4.0\"\n    retry_standoff: list = [0, 5, 10, 15, 30, 60]\n    request_timeout: int = 900\n\nclass API:\n    def __init__(self, url, api_key, api_secret, verify=False, timeout=900):\n        self._url = url\n        self._api_key = api_key\n        self._api_secret = api_secret\n        self._verify = verify\n        self._timeout = timeout\n\n    def _rest_base(self, method, api_endpoint, data=None, params=None, headers={}):\n        requests_method = getattr(requests, method)\n        response = None\n        exception = None\n        req = None\n\n        try:\n            headers['api-key'] = self._api_key\n            headers['api-secret'] = self._api_secret\n\n            if True == self._verify:\n                req = requests_method(f\"{self._url}{api_endpoint}\", timeout=self._timeout, params=params, data=json.dumps(data), headers=headers)\n            else:\n                req = requests_method(f\"{self._url}{api_endpoint}\", timeout=self._timeout, params=params, data=json.dumps(data), headers=headers, verify=self._verify)\n\n        except Exception as e:\n            exception = e\n\n        req_status_code = None\n\n        if req is not None:\n            req_status_code = req.status_code\n\n        req_json = {\"data\": \"\"}\n\n        if req is not None:\n            req_json = req.json()\n\n        return (req_status_code, req_json, exception)\n\n    def get(self, api_endpoint, data=None, params=None, headers={}):\n        return self._rest_base(\"get\", api_endpoint, data=data, params=params, headers=headers)\n\n    def post(self, api_endpoint, data=None, params=None, headers={}):\n        return self._rest_base(\"post\", api_endpoint, data=data, params=params, headers=headers)\n\nclass Metadata:\n    def __init__(self, api):\n        self._api = api\n        self._api_endpoint = \"/api/settings/metadata\"\n\n    def get_version(self):\n        status, response, exception = self._api.get(self._api_endpoint)\n\n        if status == 200 and response is not None and exception is None:\n            return response[\"Installed Version\"]\n        else:\n            raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n            \nclass Lifecycle:\n    def __init__(self, api):\n        self._api = api\n        self._api_endpoint = \"/api/dashboard/lifecycle\"\n        self._response = None\n        self._sub_phases = {}\n        self.status = None\n\n    def update(self):\n        status, response, exception = self._api.get(self._api_endpoint)\n\n        if status == 200 and response is not None and exception is None:\n             self._response = response\n             status = self._response[\"data\"][\"attributes\"][\"status\"]\n             self.status = True if \"done\" not in status else False\n\n             for sub_phase in self._response[\"data\"][\"attributes\"][\"sub_phases\"]:\n                 self._sub_phases[sub_phase[\"name\"].lower()] = True if sub_phase[\"status\"] == 1 else False\n        else:\n            raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n\n    def discovery_is_running(self):\n        if False == bool(self._sub_phases):\n            self.update()\n\n        return self.status\n\n    def correlation_is_complete(self):\n        if False == bool(self._sub_phases):\n            self.update()\n\n        return self._sub_phases[\"post_correlation\"]\n\n\nclass SavedQueries:\n    def __init__(self, api, base_api_endpoint):\n        self._api = api\n        self._api_endpoint = base_api_endpoint\n        self._queries = {}\n\n    def get_attributes_by_name(self, query_name):\n        uuid = None\n\n        if False == bool(self._queries):\n            status, response, exception = self._api.get(f\"{self._api_endpoint}/views/saved\")\n\n            if exception is not None:\n                raise Exception(exception)\n\n            for query in response[\"data\"]:\n                self._queries[query[\"attributes\"][\"name\"]] = query[\"attributes\"][\"uuid\"]\n\n        if query_name in self._queries.keys():\n            uuid = self._queries[query_name]\n\n        if uuid is not None:\n            for query in response[\"data\"]:\n                if query[\"attributes\"][\"uuid\"] == uuid:\n                    query_filter = query[\"attributes\"][\"view\"][\"query\"][\"filter\"]\n                    query_fields = query[\"attributes\"][\"view\"][\"fields\"]\n        else:\n            raise Exception(f\"Critical error: The saved query '{query_name}' does not exist\")\n\n        return (uuid, query_filter, query_fields)\n\n\nclass EntitySearch:\n    def __init__(self, api, entity_type, page_size=1000, logger_callback=None):\n        if entity_type not in [\"devices\", \"users\"]:\n            raise Exception(f\"{entity} is not a valid entity type\")\n\n        self._api = api\n        self._api_endpoint = f\"/api/{entity_type}\"\n        self._page_size = page_size\n        self._cursor = None\n        self._logger_callback = logger_callback\n        self._uuid = None\n        self._query_filter = None\n        self._query_fields = None\n\n    def _log(self, msg):\n        if self._logger_callback is not None:\n            self._logger_callback(msg)\n\n    def get(self):\n        response = { \"data\": \"init\" }\n        entities = []\n        offset = 0\n        cursor = None\n\n        while response[\"data\"]:\n            data =  {\n                \"data\": {\n                    \"type\": \"entity_request_schema\",\n                    \"attributes\": {\n                        \"page\": {\n                            \"offset\": offset,\n                            \"limit\": self._page_size\n                        },\n                        \"use_cache_entry\": False,\n                        \"always_cached_query\": False,\n                        \"get_metadata\": True,\n                        \"include_details\": True\n                    }\n                }\n            }\n\n            status, response, exception = self._api.post(self._api_endpoint, data)\n\n            if status == 200 and response is not None and exception is None:\n                for device in response[\"data\"]:\n                    entities.append(device[\"attributes\"])\n\n                offset += self._page_size\n            else:\n                raise Exception(f\"Critical Error! Status Code: {status}\\tException: {exception}\")\n\n        return entities\n\n\n    def execute_saved_query(self, name, standoff=0, shorten_field_names=False, dynamic_field_mapping={}, incremental_ingest=False, include_auids=False, truncate_fields=[], batch_callback=None):\n        try:\n            ax_saved_queries = SavedQueries(self._api, self._api_endpoint)\n\n            if self._uuid is None or self._query_filter is None or self._query_fields is None:\n                self._uuid, self._query_filter, self._query_fields = ax_saved_queries.get_attributes_by_name(name)\n\n            if True == incremental_ingest:\n                if \"specific_data.data.fetch_time\" not in self._query_fields:\n                    self._query_fields.append(\"specific_data.data.fetch_time\")\n            \n            if True == include_auids:\n                if \"internal_axon_id\" not in self._query_fields:\n                    self._query_fields.append(\"internal_axon_id\")\n\n            response = { \"data\": \"init\" }\n            entities = []\n            entity_count = 0\n\n            while response[\"data\"]:\n                data =  {\n                    \"data\": {\n                        \"type\": \"entity_request_schema\",\n                        \"attributes\": {\n                            \"use_cache_entry\": False,\n                            \"always_cached_query\": False,\n                            \"filter\": self._query_filter,\n                            \"fields\": {\n                                \"devices\": self._query_fields\n                            },\n                            \"page\": {\n                                \"limit\": self._page_size\n                            },\n                            \"get_metadata\": True,\n                            \"include_details\": True,\n                            \"use_cursor\": True,\n                            \"cursor_id\": self._cursor\n                        }\n                    }\n                }\n\n\n                status, response, exception = self._api.post(self._api_endpoint, data=data)\n\n                if status == 200 and response is not None and exception is None:\n                    if \"meta\" in response:\n                        self._cursor = response[\"meta\"][\"cursor\"]\n\n                        for device in response[\"data\"]:\n                            entity_row = {}\n\n                            for field in data['data']['attributes']['fields']['devices']:\n                                field_name = field\n\n                                if True == shorten_field_names:\n                                    field_name = field.replace(\"specific_data.data.\", \"\").replace(\"adapters_data.\", \"\")\n                                    \n                                if field_name in dynamic_field_mapping.keys():\n                                    field_name = dynamic_field_mapping[field_name]\n\n                                if field in device['attributes']:\n                                    entity_row[field_name] = device['attributes'][field]\n                                else:\n                                    entity_row[field_name] = device['attributes'][f\"{field}_details\"]\n\n                            entities.append(entity_row)\n\n                    else:\n                        response = { \"data\": None }\n                else:\n                    raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n\n                if standoff > 0:\n                    time.sleep(standoff)\n\n                if batch_callback is not None:\n                    if len(entities) > 0:\n                        batch_callback(entities)\n                        entity_count += len(entities)\n                        entities = []\n\n        except Exception as ex:\n            raise Exception(f\"Critical Error! Status Code: Exception: {ex}\")\n\nclass EventWriter:\n    def __init__(self, incremental_data_ingest=False, remove_fetch_time_field=False, checkpoint=None, host=None, source=None, index=None, sourcetype=None, helper=None, event_writer=None):\n        self._incremental_data_ingest = incremental_data_ingest\n        self._remove_fetch_time_field = remove_fetch_time_field\n        self._checkpoint = checkpoint\n        self._host = host\n        self._source = source\n        self._index = index\n        self._sourcetype = sourcetype\n        self._helper = helper\n        self._event_writer = event_writer\n        self._entity_count = 0\n        self._entity_ids = []\n        self._page = 0\n        self._events_written = 0\n\n    def process_batch(self, entities):\n        # Update entity count\n        self._entity_count += len(entities)\n\n        # Increment page number\n        self._page += 1\n\n        # Log page number and size\n        self._helper.log_info(f\"Input '{self._helper.get_arg('name')}' - STATS - Processing page {self._page}, size {len(entities)}\")\n\n        # Process each entity\n        for entity in entities:\n            if self._helper.get_arg('name') is None:\n                self._entity_ids.append(entity[\"internal_axon_id\"])\n                \n            if True == self._incremental_data_ingest:\n                # Create a timestamp from the devices fetch_time field\n                entity_fetch_time = datetime.datetime.strptime(entity[fetch_time_field_name], \"%a, %d %b %Y %H:%M:%S %Z\").timestamp()\n\n                # Remove the fetch_time field if it was not part of the saved query's query_field definition\n                if True == self._remove_fetch_time_field:\n                    entity.pop(fetch_time_field_name)\n\n                # Create event\n                event = self._helper.new_event(source=self._source, host=self._host, index=self._index, sourcetype=self._sourcetype, data=json.dumps(entity))\n\n                # Add event if no checkpoint is defined yet, or if fetch time is greater than the checkpoint time\n                if checkpoint is None:\n                    self._event_writer.write_event(event)\n                    self._events_written += 1\n                elif entity_fetch_time > checkpoint:\n                    self._event_writer.write_event(event)\n                    self._events_written += 1\n            else:\n                # Create event\n                event = self._helper.new_event(source=self._source, host=self._host, index=self._index, sourcetype=self._sourcetype, data=json.dumps(entity))\n\n                # Write event\n                self._event_writer.write_event(event)\n                self._events_written += 1\n\n    def get_entity_count(self):\n        return self._entity_count\n\n    def get_events_written(self):\n        return self._events_written\n        \n    def get_internal_axon_id_unique_count(self):\n        return len(set(self._entity_ids))\n\ndef validate_input(helper, definition):\n    # get Axonius configuration\n    api_host = definition.parameters.get('api_host', str)\n    api_key = definition.parameters.get('api_key', \"\")\n    api_secret = definition.parameters.get('api_secret', \"\")\n\n    # get selected saved query info\n    entity_type = definition.parameters.get('entity_type', str)\n    saved_query = definition.parameters.get('saved_query', str)\n\n    # get extra options\n    page_size = definition.parameters.get('page_size', str)\n    api_standoff = definition.parameters.get('standoff_ms', str)\n    ssl_certificate_path = definition.parameters.get('ssl_certificate_path', \"\")\n    enforce_ssl_validation = definition.parameters.get('enforce_ssl_validation')\n\n    try:\n        if int(page_size) < 1:\n            raise ValueError(\"Page Size must be an integer greater than 0\")\n\n        if int(api_standoff) < 0:\n            raise ValueError(\"API Standoff must be an integer greater or equal to 0\")\n\n    except Exception as ex:\n        raise ValueError(ex)\n\n    # Create api object\n    try:\n        verify = True\n\n        helper.log_info(f\"enforce_ssl_validation: {enforce_ssl_validation}\")\n\n        if str(enforce_ssl_validation).lower() not in [\"1\", \"true\"]:\n            verify = False\n\n        helper.log_info(f\"verify: {verify}\")\n\n        if ssl_certificate_path is not None:\n            if len(ssl_certificate_path) > 0:\n                verify = ssl_certificate_path\n\n        api = API(api_host, str(api_key), str(api_secret), verify)\n        search = EntitySearch(api, \"devices\", 1000)\n        out = search.get()\n    except Exception as ex:\n        helper.log_info(ex)\n\n        if \"Could not find a suitable TLS CA certificate bundle\" in str(ex):\n            raise ValueError(\"Critical Error, check CA Bundle Path exists and the splunk user has proper permissions\")\n        elif \"SSLCertVerificationError\" in str(ex) or \"Could not find a suitable TLS CA certificate bundle\" in str(ex):\n            raise ValueError(\"The Axonius host fails SSL verification, please review your SSL certificate validation settings\")\n        elif \"Status Code: 401\" not in str(ex):\n            raise ValueError(f\"Critical Error: {ex}\")\n\n    pass\n\ndef collect_events(helper, ew):\n    # Retrieve checkpoint\n    checkpoint_name = f\"checkpoint_{helper.get_arg('name')}_{helper.get_arg('entity_type')}_{helper.get_arg('saved_query')}\"\n\n    # get Axonius configuration\n    opt_api_host = helper.get_arg('api_host')\n    opt_api_key = helper.get_arg('api_key')\n    opt_api_secret = helper.get_arg('api_secret')\n\n    # get selected saved query info\n    opt_entity_type = helper.get_arg('entity_type')\n    opt_saved_query = helper.get_arg('saved_query')\n\n    # get extra options\n    opt_page_size = helper.get_arg('page_size')\n    opt_shorten_field_names = helper.get_arg('shorten_field_names')\n    opt_incremental_data_ingest = helper.get_arg('incremental_data_ingest')\n    opt_standoff_ms = helper.get_arg('standoff_ms')\n    opt_field_mapping = helper.get_arg('dynamic_field_mapping')\n    opt_ssl_certificate_path = helper.get_arg('ssl_certificate_path')\n    opt_enforce_ssl_validation = helper.get_arg('enforce_ssl_validation')\n    \n    # Logging functions\n    def log_info(msg):\n        helper.log_info(f\"Input '{helper.get_arg('name')}' - {msg}\")\n        \n    def log_warning(msg):\n        helper.log_warning(f\"Input '{helper.get_arg('name')}' - {msg}\")\n        \n    def log_error(msg):\n        helper.log_error(f\"Input '{helper.get_arg('name')}' - {msg}\")\n        \n    def log_critical(msg):\n        helper.log_critical(f\"Input '{helper.get_arg('name')}' - {msg}\")\n    \n    # Log input variables\n    log_info(f\"VARS - Axonius Host: {opt_page_size}\")\n    log_info(f\"VARS - Entity type: {opt_entity_type}\")\n    log_info(f\"VARS - Saved query: {opt_saved_query}\")\n    log_info(f\"VARS - Page size: {opt_page_size}\")\n    log_info(f\"VARS - Shorten field names: {opt_shorten_field_names}\")\n    log_info(f\"VARS - Incremental data ingest: {opt_incremental_data_ingest}\")\n    log_info(f\"VARS - API standoff (ms): {opt_standoff_ms}\")\n    log_info(f\"VARS - Field Mapping: {opt_field_mapping}\")\n    log_info(f\"VARS - Enforce SSL validation: {opt_enforce_ssl_validation}\")\n    log_info(f\"VARS - CA bundle path: {opt_ssl_certificate_path}\")\n    \n    include_auids = True if helper.get_arg('name') is None else False\n    critical_error = False\n\n    # Set verify to True/False\n    verify = opt_enforce_ssl_validation\n\n    # Change the value of verify to the path of the ca_bundle if specified\n    if opt_ssl_certificate_path:\n        if len(opt_ssl_certificate_path) > 0:\n            verify = opt_ssl_certificate_path\n\n    # The host field will be used to set the source host in search\n    host = None\n\n    # Pull out just the host information from the Host\n    match = re.match(\"(?:https?:\\/\\/)([0-9A-z-.]+)(?::\\d+)?\", opt_api_host)\n\n    # Only set host if the regex exists, match should never be None.\n    if match is not None:\n        host=match.groups()[0]\n\n    timeout = Config.request_timeout if helper.get_arg('name') is not None else 5\n    retry_standoff = Config.retry_standoff if helper.get_arg('name') is not None else [0, 3, 3, 3]\n\n    # Create an API object for REST calls\n    api = API(opt_api_host, opt_api_key, opt_api_secret, verify, timeout=timeout)\n\n    # Create EntitySearch object with entity type and page size\n    search = EntitySearch(api, opt_entity_type, opt_page_size, log_info)\n\n    # Load the input's checkpoint data\n    checkpoint = helper.get_check_point(checkpoint_name)\n\n    if checkpoint is not None:\n        log_info(f\"VARS - Check point: {checkpoint_name}\")\n\n    # Default dynamic field names to an empty dict in case opt_field_mapping is empty\n    dynamic_field_names = {}\n\n    # Use dynamic mapping if specified\n    if opt_field_mapping is not None:\n        if len(opt_field_mapping) > 0:\n            try:\n                dynamic_field_names = json.loads(opt_field_mapping)\n            except Exception as ex:\n                pass\n\n    # Retry variables\n    fetch_complete = False\n    exception_thrown = False\n    max_retries = len(retry_standoff)\n    entity_count = 0\n    retries = 0\n    version = None\n    event_writer = None\n    lifecycle_complete = False\n\n    # Set the fetch_time field name, take into account the use of shorten field name\n    fetch_time_field_name = \"fetch_time\" if True == opt_shorten_field_names else \"specific_data.data.fetch_time\"\n\n    while retries < max_retries and not True == critical_error and not True == fetch_complete:\n        try:\n            if version is None:\n                # Get the raw Axonius version from the metadata endpoint\n                metadata = Metadata(api)\n                version = metadata.get_version()\n    \n                # Pull out just the host information from the Host\n                match = re.match(\"(\\d+\\_\\d+\\_\\d+)(?:_RC\\d+)\", version)\n    \n                # Only set host if the regex exists, match should never be None.\n                if match is not None:\n                    version = match.groups()[0].replace(\"_\", \".\")\n    \n                log_info(f\"STATS - Version: {version}\")\n                \n                # Turn versions into tuples for equality comparison\n                tup_version = tuple(map(int, (version.split(\".\"))))\n                tup_supported_version = tuple(map(int, (Config.supported_minimum_version.split(\".\"))))\n                \n                # If the current version is less than supported, throw a critical exception\n                if tup_version < tup_supported_version:\n                    raise Exception(\"UnsupportedVersion\")\n                    \n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n            \n            if not True == lifecycle_complete:\n                # Check if a discovery is running and correlation hasn't complete, warn customer if true\n                lifecycle = Lifecycle(api)\n                \n                if True == lifecycle.discovery_is_running() and not True == lifecycle.correlation_is_complete():\n                    log_warning(f\"Warning: Fetch started while correlation was not complete.\")\n                    \n                lifecycle_complete = True\n                \n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n                    \n            if event_writer is None:\n                # Get definition of query_fields, used to check if the fetch_time field should be removed\n                api_endpoint = f\"/api/{opt_entity_type}\"\n                ax_saved_queries = SavedQueries(api, api_endpoint)\n                uuid, query_filter, query_fields = ax_saved_queries.get_attributes_by_name(opt_saved_query)\n    \n                # Default remove fetch time to true\n                remove_fetch_time_field = True\n    \n                # Look for fetch_time in the query_fields definition of the specified saved query\n                if True == opt_shorten_field_names:\n                    if fetch_time_field_name in query_fields:\n                        remove_fetch_time_field = False\n    \n                # Create EventWriter instance to process batches\n                event_writer = EventWriter(incremental_data_ingest=opt_incremental_data_ingest, remove_fetch_time_field=remove_fetch_time_field, checkpoint=checkpoint, host=host, source=helper.get_arg('name'), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), helper=helper, event_writer=ew)\n            \n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n\n            # Grab entity from the saved search\n            search.execute_saved_query(opt_saved_query, int(opt_standoff_ms)/1000, opt_shorten_field_names, dynamic_field_names, incremental_ingest=opt_incremental_data_ingest, include_auids=include_auids, batch_callback=event_writer.process_batch)\n\n            # Get Stats\n            entity_count = event_writer.get_entity_count()\n            events_written = event_writer.get_events_written()\n            \n            # Fetch is complete, see below for consistency checks if an exception was thrown during fetch\n            fetch_complete = True\n\n            # Log stats\n            log_info(f\"STATS - Total entities returned: {entity_count}\")\n            log_info(f\"STATS - Total events written: {events_written}\")\n            \n            # Sanity check for unique ids, the number needs to match entity_count\n            if helper.get_arg('name') is None:\n                log_info(f\"STATS - Total unique ids: {event_writer.get_internal_axon_id_unique_count()}\")\n        except Exception as ex:\n            # Die if running an unsupported version of Axonius, or log the error and track for retry purposes\n            if \"UnsupportedVersion\" in str(ex):\n                critical_error = True\n            else:\n                log_error(f\"ERR - Error '{ex}'\")\n                exception_thrown = True\n\n        if True == critical_error:\n            log_critical(f\"Critical Error: Axonius version {version} is unsupported, the minimum version is {Config.supported_minimum_version}\")\n        elif True == exception_thrown and not True == fetch_complete:\n            # Increment retry counter\n            retries += 1\n\n            if retries < max_retries:\n                #Log retry number and display the standoff\n                log_info(f\"COLL - Retry {retries} sleeping for {retry_standoff[retries]} seconds, then retrying\")\n\n                # Sleep the process and then retry\n                time.sleep(retry_standoff[retries])\n            else:\n                # Log no devices after max retries\n                log_critical(f\"Critical Error: Unable to complete fetch due to unrecoverable errors.\")\n        elif True == exception_thrown and True == fetch_complete:\n            # Log recovered from error during fetch\n            log_warning(f\"Warning: Fetch was interrupted by a transient error, review results for fetch completeness.\")\n        else:\n            # Save new checkpoint if entity_count is greater than one\n            if entity_count > 0:\n                helper.save_check_point(checkpoint_name, datetime.datetime.now().timestamp())\n", "customized_options": [{"name": "api_host", "value": "https://demo-latest.axonius.com"}, {"name": "entity_type", "value": "devices"}, {"name": "saved_query", "value": "Ax - Get all devices"}, {"name": "page_size", "value": "1000"}, {"name": "standoff_ms", "value": "0"}, {"name": "dynamic_field_mapping", "value": "{\"hostname\": \"host\", \"network_interfaces.manufacturer\": \"nic_manufacturer\"}"}, {"name": "shorten_field_names", "value": true}, {"name": "incremental_data_ingest", "value": false}, {"name": "enforce_ssl_validation", "value": true}, {"name": "ssl_certificate_path", "value": ""}], "uuid": "2ac09a9bc4b14b08843d0a650215367e", "sample_count": "37234"}]}, "field_extraction_builder": {"axonius_saved_query": {"is_parsed": true, "data_format": "json"}}, "global_settings_builder": {"global_settings": {"log_settings": {"log_level": "DEBUG"}}}, "sourcetype_builder": {"axonius_saved_query": {"metadata": {"data_input_name": "axonius_saved_query"}}}, "validation": {}}